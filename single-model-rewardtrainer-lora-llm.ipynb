{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, we will be training, validating, and inferring using Deberta Large along with the Low-Rank Adaptation technique.\n\nI am experimenting with a slightly different approach called Reward Modelling, and it has been yielding decent results thus far. Essentially, it involves following these steps:\n- Creating a new dataset format consisting of rejected and chosen examples for each option, essentially utilizing the hh-rlhf format.\n- Feeding these instances into the reward training pipeline.\n- Using a sequence classification head to predict logits for each pair.\n- Sorting each answer by its probability and selecting the top 3.\n\nFor training, I utilized [Radek's awesome dataset](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam) and created some experimental dataset using llama-2-13b for reward training [here](https://www.kaggle.com/datasets/datafan07/rlhf-data-for-llm-science-exam). I believe that with larger datasets, the scores could be improved further, considering that lora training is quite fast, which should not pose a significant challenge for training. With the lora approach, I managed to train over 30k instances in under 5 minutes using a retail graphics card.\n\nSo let's get started...","metadata":{}},{"cell_type":"code","source":"# installing offline dependencies\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using single gpu for this case\n\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:11.950119Z","iopub.execute_input":"2023-08-10T13:25:11.951054Z","iopub.status.idle":"2023-08-10T13:25:11.961259Z","shell.execute_reply.started":"2023-08-10T13:25:11.951006Z","shell.execute_reply":"2023-08-10T13:25:11.960244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom datasets import load_dataset, Dataset\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel, PeftConfig, TaskType, PeftModelForSequenceClassification\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModel,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\nimport random\nimport os\nimport gc\n\n\n\nfrom trl import RewardTrainer\n\nimport pandas as pd\npd.set_option('display.max_colwidth', None)\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:12.129893Z","iopub.execute_input":"2023-08-10T13:25:12.130558Z","iopub.status.idle":"2023-08-10T13:25:18.624492Z","shell.execute_reply.started":"2023-08-10T13:25:12.130525Z","shell.execute_reply":"2023-08-10T13:25:18.623467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    base_model = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-large\"\n    seed = 42","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:18.626358Z","iopub.execute_input":"2023-08-10T13:25:18.627266Z","iopub.status.idle":"2023-08-10T13:25:18.631756Z","shell.execute_reply.started":"2023-08-10T13:25:18.627229Z","shell.execute_reply":"2023-08-10T13:25:18.630731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = int):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\n\nrandom_state = set_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:18.633254Z","iopub.execute_input":"2023-08-10T13:25:18.634160Z","iopub.status.idle":"2023-08-10T13:25:18.647859Z","shell.execute_reply.started":"2023-08-10T13:25:18.634125Z","shell.execute_reply":"2023-08-10T13:25:18.646844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading multi option dataset\n\ntrain_df_0 = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/6000_train_examples.csv')\ntrain_df_1 = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/extra_train_set.csv')\ntest_df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv')\n\n\n# merge and drop empty lines\n\ntrain_df = pd.concat((train_df_0, train_df_1), axis=0)\ntrain_df.dropna(inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:18.652010Z","iopub.execute_input":"2023-08-10T13:25:18.652292Z","iopub.status.idle":"2023-08-10T13:25:18.722436Z","shell.execute_reply.started":"2023-08-10T13:25:18.652268Z","shell.execute_reply":"2023-08-10T13:25:18.721512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To prepare our dataset for the reward training task, we concatenate each question with its corresponding answer option, creating new pairs for each prompt. The chosen column represents the preferred answer, while the rejected column contains the incorrect answers. This approach enables our model to compare each pair effectively.","metadata":{}},{"cell_type":"code","source":"def generate_new_dataframe(df):\n    new_rows = []\n\n    # Iterate through each row in the original DataFrame\n    for _, row in df.iterrows():\n        prompt = row['prompt']\n        chosen_option = row[row['answer']]  # Get the text of the chosen option based on the 'answer' column\n\n        # Iterate through each option\n        for option in ['A', 'B', 'C', 'D', 'E']:\n            if option != row['answer']:\n                rejected_option = row[option]  # Get the text of the rejected option\n                new_row = {'chosen': prompt + ' ' + chosen_option, 'rejected': prompt + ' ' + rejected_option}\n                new_rows.append(new_row)\n\n    # Create a new DataFrame from the new_rows list\n    new_df = pd.DataFrame(new_rows)\n    return new_df\n\n\ntrain_df = generate_new_dataframe(train_df)\ntest_df = generate_new_dataframe(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:18.761696Z","iopub.execute_input":"2023-08-10T13:25:18.762039Z","iopub.status.idle":"2023-08-10T13:25:19.518086Z","shell.execute_reply.started":"2023-08-10T13:25:18.762007Z","shell.execute_reply":"2023-08-10T13:25:19.517089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding extra reward data\n\ntrain_df_2 = pd.read_csv('/kaggle/input/rlhf-data-for-llm-science-exam/llm_rlhf_extra.csv')\ntrain_df = pd.concat((train_df, train_df_2), axis=0)\ntrain_df = train_df.sample(frac=1.0, random_state=CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:19.519693Z","iopub.execute_input":"2023-08-10T13:25:19.520086Z","iopub.status.idle":"2023-08-10T13:25:19.554387Z","shell.execute_reply.started":"2023-08-10T13:25:19.520051Z","shell.execute_reply":"2023-08-10T13:25:19.553433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting to dataset format\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:19.555934Z","iopub.execute_input":"2023-08-10T13:25:19.556323Z","iopub.status.idle":"2023-08-10T13:25:19.599650Z","shell.execute_reply.started":"2023-08-10T13:25:19.556287Z","shell.execute_reply":"2023-08-10T13:25:19.598621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading base model and tokenizers\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    CFG.base_model,\n    num_labels=1,\n\n)\n\ntokenizer = AutoTokenizer.from_pretrained(CFG.base_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:19.603600Z","iopub.execute_input":"2023-08-10T13:25:19.603885Z","iopub.status.idle":"2023-08-10T13:25:27.445144Z","shell.execute_reply.started":"2023-08-10T13:25:19.603859Z","shell.execute_reply":"2023-08-10T13:25:27.443782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is our lora config. With a relatively small rank, our model should be quite resource efficient during training.","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1, \n    bias=\"none\", inference_mode=False, target_modules=[\"query_proj\", \"value_proj\"]\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:27.447063Z","iopub.execute_input":"2023-08-10T13:25:27.447743Z","iopub.status.idle":"2023-08-10T13:25:27.453380Z","shell.execute_reply.started":"2023-08-10T13:25:27.447705Z","shell.execute_reply":"2023-08-10T13:25:27.452237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see we're going to train around 0.18% of actual model parameters.","metadata":{}},{"cell_type":"code","source":"model = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:27.455258Z","iopub.execute_input":"2023-08-10T13:25:27.455984Z","iopub.status.idle":"2023-08-10T13:25:28.167496Z","shell.execute_reply.started":"2023-08-10T13:25:27.455949Z","shell.execute_reply":"2023-08-10T13:25:28.166427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we transform our dataset into the expected format for the RewardTrainer. Additionally, we exclude instances where the sequence length exceeds 256.","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    new_examples = {\n        \"input_ids_chosen\": [],\n        \"attention_mask_chosen\": [],\n        \"input_ids_rejected\": [],\n        \"attention_mask_rejected\": [],\n    }\n    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n        tokenized_j = tokenizer(chosen, truncation=True)\n        tokenized_k = tokenizer(rejected, truncation=True)\n\n        new_examples[\"input_ids_chosen\"].append(tokenized_j[\"input_ids\"])\n        new_examples[\"attention_mask_chosen\"].append(tokenized_j[\"attention_mask\"])\n        new_examples[\"input_ids_rejected\"].append(tokenized_k[\"input_ids\"])\n        new_examples[\"attention_mask_rejected\"].append(tokenized_k[\"attention_mask\"])\n\n    return new_examples\n\ntrain_dataset = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=4,\n)\ntrain_dataset = train_dataset.filter(\n    lambda x: len(x[\"input_ids_chosen\"]) <= 256\n    and len(x[\"input_ids_rejected\"]) <= 256\n)\n\n\ntest_dataset = test_dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=4,\n)\ntest_dataset = test_dataset.filter(\n    lambda x: len(x[\"input_ids_chosen\"]) <= 2048\n    and len(x[\"input_ids_rejected\"]) <= 2048\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:28.168857Z","iopub.execute_input":"2023-08-10T13:25:28.169544Z","iopub.status.idle":"2023-08-10T13:25:44.624549Z","shell.execute_reply.started":"2023-08-10T13:25:28.169509Z","shell.execute_reply":"2023-08-10T13:25:44.623355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will select specific settings, making slight alterations to accommodate Kaggle notebook limitations. This includes utilizing adafactor optimization and implementing gradient accumulation to conserve some memory.","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='op',\n    overwrite_output_dir = True,\n    warmup_ratio=0.1,\n    lr_scheduler_type='cosine',\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=1,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    remove_unused_columns=False,\n    optim=\"adafactor\",\n    logging_steps=250,\n    eval_steps=250,\n    evaluation_strategy='steps',\n    load_best_model_at_end=True,\n    save_total_limit = 2,\n    fp16=True,\n    bf16=False,\n    weight_decay=0.01,\n    report_to=\"none\",\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:44.626493Z","iopub.execute_input":"2023-08-10T13:25:44.627149Z","iopub.status.idle":"2023-08-10T13:25:44.636423Z","shell.execute_reply.started":"2023-08-10T13:25:44.627113Z","shell.execute_reply":"2023-08-10T13:25:44.635303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we set our RewardTrainer, I am going to use actual competition training data for evaluation while external generated data for training.","metadata":{}},{"cell_type":"code","source":"trainer = RewardTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_config,\n    max_length=256,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:44.639280Z","iopub.execute_input":"2023-08-10T13:25:44.639967Z","iopub.status.idle":"2023-08-10T13:25:44.723715Z","shell.execute_reply.started":"2023-08-10T13:25:44.639928Z","shell.execute_reply":"2023-08-10T13:25:44.722663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.save_model('deberta_adapter')\n\ndel model\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:25:44.724957Z","iopub.execute_input":"2023-08-10T13:25:44.725312Z","iopub.status.idle":"2023-08-10T13:54:07.278950Z","shell.execute_reply.started":"2023-08-10T13:25:44.725277Z","shell.execute_reply":"2023-08-10T13:54:07.277800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading original training data for evaluation.\ndf = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:54:07.281162Z","iopub.execute_input":"2023-08-10T13:54:07.282110Z","iopub.status.idle":"2023-08-10T13:54:07.295510Z","shell.execute_reply.started":"2023-08-10T13:54:07.282074Z","shell.execute_reply":"2023-08-10T13:54:07.294525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this stage, we have a sequence classifier for each question-answer pair, which calculates the likelihood of how well each answer option fits with the given question. This process is performed for every pair, and we then select the top 3 pairs with the highest likelihood scores.","metadata":{}},{"cell_type":"code","source":"def get_score(model, tokenizer, prompt, response):\n    inputs = tokenizer(prompt + ' ' + response, return_tensors=\"pt\", max_length=2048, padding='longest', truncation=True).to('cuda')\n    model.to('cuda')\n    model.eval()\n    with torch.autocast('cuda', dtype=torch.float16):\n        outputs = model(input_ids = inputs['input_ids'], attention_mask=inputs['attention_mask'])\n    logits = outputs.logits\n\n    return logits.item()\n\ndef get_top_3_winners(model, tokenizer, prompt, response_options):\n    scores = []\n    for index, response in enumerate(response_options):\n        score = get_score(model, tokenizer, prompt, response)\n        scores.append((index, score))\n\n    \n    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    \n    top_3_winners = sorted_scores[:3]\n    top_3_winners = [t[0] for t in top_3_winners]\n\n    int_to_string = {\n    0: 'A',\n    1: 'B',\n    2: 'C',\n    3: 'D',\n    4: 'E'\n    }\n\n    top_3_winners = [int_to_string[val] for val in top_3_winners]\n\n    \n    return top_3_winners","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:54:07.297262Z","iopub.execute_input":"2023-08-10T13:54:07.297653Z","iopub.status.idle":"2023-08-10T13:54:07.310412Z","shell.execute_reply.started":"2023-08-10T13:54:07.297616Z","shell.execute_reply":"2023-08-10T13:54:07.309226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor _, row in tqdm(df.iterrows()):\n    prompt = row['prompt']\n    response_options = [\n        row['A'],\n        row['B'],\n        row['C'],\n        row['D'],\n        row['E']\n    ]\n    top_3_winners = get_top_3_winners(trainer.model, tokenizer, prompt, response_options)\n    preds.append(top_3_winners)\n    \nfinal_preds = [' '.join(pred) for pred in preds]","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:54:07.312243Z","iopub.execute_input":"2023-08-10T13:54:07.314006Z","iopub.status.idle":"2023-08-10T13:56:02.391284Z","shell.execute_reply.started":"2023-08-10T13:54:07.313979Z","shell.execute_reply":"2023-08-10T13:56:02.390279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n\ndef precision_at_k(r, k):\n    \"\"\"Precision at k\"\"\"\n    assert k <= len(r)\n    assert k != 0\n    return sum(int(x) for x in r[:k]) / k\n\ndef MAP_at_3(predictions, true_items):\n    \"\"\"Score is mean average precision at 3\"\"\"\n    U = len(predictions)\n    map_at_3 = 0.0\n    for u in range(U):\n        user_preds = predictions[u]\n        user_true = true_items[u]\n        user_results = [1 if item == user_true else 0 for item in user_preds]\n        for k in range(min(len(user_preds), 3)):\n            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n    return map_at_3 / U\n\n\nMAP_at_3(final_preds, df['answer'])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:56:02.405126Z","iopub.execute_input":"2023-08-10T13:56:02.405427Z","iopub.status.idle":"2023-08-10T13:56:02.421862Z","shell.execute_reply.started":"2023-08-10T13:56:02.405397Z","shell.execute_reply":"2023-08-10T13:56:02.420734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems promising score, let's predict the competition data and make a submission.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')\npreds = []\nfor _, row in tqdm(df.iterrows()):\n    prompt = row['prompt']\n    response_options = [\n        row['A'],\n        row['B'],\n        row['C'],\n        row['D'],\n        row['E']\n    ]\n    top_3_winners = get_top_3_winners(trainer.model, tokenizer, prompt, response_options)\n    preds.append(top_3_winners)\nfinal_preds = [' '.join(pred) for pred in preds]","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:56:02.423308Z","iopub.execute_input":"2023-08-10T13:56:02.424062Z","iopub.status.idle":"2023-08-10T13:57:56.435370Z","shell.execute_reply.started":"2023-08-10T13:56:02.424027Z","shell.execute_reply":"2023-08-10T13:57:56.434275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/sample_submission.csv')\nsub['prediction'] = final_preds","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:57:56.669371Z","iopub.execute_input":"2023-08-10T13:57:56.669709Z","iopub.status.idle":"2023-08-10T13:57:56.679903Z","shell.execute_reply.started":"2023-08-10T13:57:56.669681Z","shell.execute_reply":"2023-08-10T13:57:56.678955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)\npd.read_csv('submission.csv').head()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:57:56.681365Z","iopub.execute_input":"2023-08-10T13:57:56.681873Z","iopub.status.idle":"2023-08-10T13:57:56.705884Z","shell.execute_reply.started":"2023-08-10T13:57:56.681838Z","shell.execute_reply":"2023-08-10T13:57:56.704924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I tried to come up with some alternate approach, aiming to add variety. I hope you find this baseline solution helpful, and if you do, kindly consider giving it an upvote. Thank you!","metadata":{}}]}