{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-11T13:54:31.005967Z","iopub.execute_input":"2023-09-11T13:54:31.006331Z","iopub.status.idle":"2023-09-11T13:54:31.014908Z","shell.execute_reply.started":"2023-09-11T13:54:31.006303Z","shell.execute_reply":"2023-09-11T13:54:31.013799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This notebook's description  \nIn this notebook, the OpenAI's gpt3.5-turbo model was utilized to create prompts in a very simple manner (with minimal data manipulation), organize predicted values for each record, and calculate accuracy. For those who have not created prompts before, the section **\"create prompt and request openai gpt3.5 turbo to solve problems\"** provides guidance on prompt creation which could be helpful.  \n\n**There are two points to note:  \n1.To execute this notebook in its entirety, you will need to obtain an API key from OpenAI.  \n2.This notebook is an attempt to use the results of gpt3.5 turbo as a personal benchmark. Even if you execute all the steps here, please be aware that it cannot be submitted.**\n\nThe gpt model generates responses in various forms, making their control challenging. Therefore, to stabilize the variation in response generation, it is effective to include phrases in the prompts that instruct the desired response format. Additionally, I believe it's important to adjust the value of the \"temperature\" parameter, which is one of the factors influencing response generation.  \n\nLastly, thank you very much for reading up to this point! If you found this helpful in any way, I would greatly appreciate it if you could upvote.  \n\n**The following text has been added as an update on August 29th.**  \nAdditional methods to increase the percentage of correct answers were considered.  \nPlease take a look at since the section entitled **\"In addition to GPT-3.5, I will attempt improvements by utilizing the Wikipedia API\"**, as it yielded a somewhat higher percentage of correct answers than simply getting the answers from GPT 3.5!  \n\n**The following text has been added as an update on September 5.**  \nAdditional methods to increase the percentage of correct answers were considered.\nPlease take a look at since the section entitled \"**Extract wikipedia information related to a question using the langchain library**\".Using the langchain library, I investigated ways to extract information that is highly relevant to the question, and including this in the prompt and generating responses with GPT 3.5 further increased the percentage of correct responses!","metadata":{}},{"cell_type":"markdown","source":"# check data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\")\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T13:54:34.793649Z","iopub.execute_input":"2023-09-11T13:54:34.794032Z","iopub.status.idle":"2023-09-11T13:54:34.845040Z","shell.execute_reply.started":"2023-09-11T13:54:34.794002Z","shell.execute_reply":"2023-09-11T13:54:34.844048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nprint(\"prompt question is: \",train.loc[i,\"prompt\"])\nprint(\"----------------------------------\")\nfor option in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n    print(\"Option \",option,\": \",train.loc[i,option])\n    print(\"----------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T13:54:35.056472Z","iopub.execute_input":"2023-09-11T13:54:35.056829Z","iopub.status.idle":"2023-09-11T13:54:35.064290Z","shell.execute_reply.started":"2023-09-11T13:54:35.056798Z","shell.execute_reply":"2023-09-11T13:54:35.063188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, this seems like a challenging problem indeed. It doesn't seem like something I could answer off the top of my head either.","metadata":{}},{"cell_type":"markdown","source":"# install library openai  \nto request gpt3.5 turbo we need install openai library","metadata":{}},{"cell_type":"code","source":"!pip install openai","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-11T13:54:46.022340Z","iopub.execute_input":"2023-09-11T13:54:46.022696Z","iopub.status.idle":"2023-09-11T13:54:58.719922Z","shell.execute_reply.started":"2023-09-11T13:54:46.022669Z","shell.execute_reply":"2023-09-11T13:54:58.718636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# set OpenAI API-key  \nIn this notebook, I will be using OpenAI's API key. Once defined as a variable below, you will specify the API key from the variable 'apikey.' To obtain an OpenAI API key, you can create an account on OpenAI's official website: https://openai.com/ and generate it from your profile page.","metadata":{}},{"cell_type":"code","source":"apikey=\"your-api-key\"","metadata":{"execution":{"iopub.status.busy":"2023-09-11T13:54:58.721988Z","iopub.execute_input":"2023-09-11T13:54:58.722334Z","iopub.status.idle":"2023-09-11T13:54:58.728181Z","shell.execute_reply.started":"2023-09-11T13:54:58.722301Z","shell.execute_reply":"2023-09-11T13:54:58.726620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# create prompt and request openai gpt3.5 turbo to solve problems","metadata":{}},{"cell_type":"code","source":"#Creates a generic GPT calling function\ndef request_gpt(model_name,messages_list):\n    response=openai.ChatCompletion.create(\n        model=model_name,\n        messages=messages_list,\n        temperature =0\n        )\n    return response","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:00:01.294106Z","iopub.execute_input":"2023-09-11T14:00:01.294494Z","iopub.status.idle":"2023-09-11T14:00:01.300253Z","shell.execute_reply.started":"2023-09-11T14:00:01.294463Z","shell.execute_reply":"2023-09-11T14:00:01.299171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport openai\nimport time\n\n# you cat get openai apikey when you create account\nopenai.api_key = apikey\nanswer_list=[]\n\nfor i in range(train.shape[0]):\n    prompt_str=train.loc[i,\"prompt\"]+' Please select the most accurate option from the choices A to E above and answer just like \"The answer is A\" \\\\n----------------------------------\\\\n'\n    for option in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n        prompt_str=prompt_str+'Option ' + option + ' : ' + train.loc[i,option] + '\\\\n'\n    messages=[{\"role\": \"user\", \"content\": prompt_str}]\n    #Sometimes errors occur, so try&catch so that you can retry once\n    try:\n        response=request_gpt(\"gpt-3.5-turbo\",messages)\n    except:\n        response=request_gpt(\"gpt-3.5-turbo\",messages)\n    if i%25==0:\n        #print sample responce\n        print(\"id\",str(i),\" responce is : \",response[\"choices\"][0][\"message\"][\"content\"])\n        #Progress check\n    answer_list.append(response[\"choices\"][0][\"message\"][\"content\"][14])\n    #There is a 1-minute request limit with OpenAI, so please wait for 3 seconds between each request.\n    #Reference information:https://platform.openai.com/docs/guides/rate-limits/overview\n    time.sleep(1)\nprint(\"done\")\n\ntrain[\"prediction\"]=answer_list\n#Let's take a look at the answers for the first 10 questions\nanswer_list[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:00:03.441963Z","iopub.execute_input":"2023-09-11T14:00:03.442312Z","iopub.status.idle":"2023-09-11T14:09:12.650432Z","shell.execute_reply.started":"2023-09-11T14:00:03.442287Z","shell.execute_reply":"2023-09-11T14:09:12.649284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check accuracy score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(\"accuracy score is : \",accuracy_score(train[\"answer\"],train[\"prediction\"]))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:09:12.652580Z","iopub.execute_input":"2023-09-11T14:09:12.653408Z","iopub.status.idle":"2023-09-11T14:09:13.189242Z","shell.execute_reply.started":"2023-09-11T14:09:12.653371Z","shell.execute_reply":"2023-09-11T14:09:13.187985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reflection-1  \nIt's not too bad to have a about 70 percent accuracy rate considering there are five multiple-choice questions. However, given that you're using gpt3.5-turbo, the performance could indeed be better.  \n\n\n**The following text has been added as an update on August 29th.**  \nSo, it may sound simplistic, but I considered adding additional features.  \nAs for where to obtain information, I turned to Wikipedia.  \nFortunately, retrieving Wikipedia information isn't overly challenging, with APIs and libraries readily available.  \nHowever, to fetch Wikipedia information, specific search keywords are required for each question.  \nAs these are unavailable, I've decided to have GPT-3.5 extract search keywords for me.","metadata":{}},{"cell_type":"markdown","source":"# install library wikipedia\nto access and parse data from Wikipedia, we need install wikipedia library","metadata":{}},{"cell_type":"code","source":"!pip install wikipedia","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-11T14:09:13.190626Z","iopub.execute_input":"2023-09-11T14:09:13.190973Z","iopub.status.idle":"2023-09-11T14:09:26.735291Z","shell.execute_reply.started":"2023-09-11T14:09:13.190944Z","shell.execute_reply":"2023-09-11T14:09:26.733703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I will attempt improvements by utilizing the Wikipedia API.","metadata":{}},{"cell_type":"code","source":"import wikipedia\nwiki_info=[]\nfor i in range(train.shape[0]):\n    #Generate keywords for searching on Wikipedia using the question information and GPT.\n    prompt_str=train.loc[i,\"prompt\"]\n    messages=[]\n    messages.append({\"role\": \"user\", \"content\": prompt_str})\n    messages.append({\"role\": \"user\", \"content\": 'Create one most important keyword for searching the above question on Wikipedia. Keyword only, no explanations or additional text.'})\n    \n    try:\n        response=response=request_gpt(\"gpt-3.5-turbo\",messages)\n    except:\n        response=response=request_gpt(\"gpt-3.5-turbo\",messages)\n\n    keyword = response[\"choices\"][0][\"message\"][\"content\"]\n    #Sometimes, there might be quotation marks at the beginning and end of the generated GPT text, so I will exclude these using slicing.\n    if (keyword[0]=='\"\"')&(keyword[-1]=='\"\"'):\n        keyword=keyword[1:-1]\n    \n    #utilizing the Wikipedia API\n    wikipedia.set_lang(\"en\")\n    search_response = wikipedia.search(keyword[:300])\n    info_str=\"\"\n    page_data=\"\"\n    #The wikipedia.search method retrieves a list of information available on Wikipedia through execution.\n    #However, there are occasional instances where the result is zero. In such cases, the addition of information is abandoned.\n    if len(search_response)==0:\n        info_str = \"\"\n    else:\n        #Sometimes, when the target page doesn't exist, I give up on adding the information.\n        try:\n            page_data = wikipedia.page(search_response[0])\n            info_str = page_data.content\n        except:\n            try:\n                if len(search_response)>1:\n                    page_data = wikipedia.page(search_response[1])\n                    info_str = page_data.content\n                else:\n                    info_str = \"\"\n            except:\n                info_str = \"\"\n    wiki_info.append(info_str)\n    \n    #Display a sample of each result.\n    if i%50==0:\n        print(\"id is : \",str(i))\n        print(\"prompt message is : \" , messages)\n        print('gpt3.5turbo answer(created keyword) : \\n',response[\"choices\"][0][\"message\"][\"content\"])\n        print('wikipedia search result : \\n',search_response)\n        print('wikipedia page content(Due to the large volume, only the first 200 characters will be displayed) : \\n',info_str[:100])\n        print()\n\n    time.sleep(1)\n\n#Add the information obtained from Wikipedia as a feature column.\ntrain[\"wiki_info\"]=wiki_info\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:13:07.127785Z","iopub.execute_input":"2023-09-11T14:13:07.128207Z","iopub.status.idle":"2023-09-11T14:28:02.935675Z","shell.execute_reply.started":"2023-09-11T14:13:07.128165Z","shell.execute_reply":"2023-09-11T14:28:02.934338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The amount of information that can be retrieved from wikipedia seems to vary from line to line.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nprint(train[\"wiki_info\"].str.len().describe())\nsns.histplot(data=train[\"wiki_info\"].str.len())","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:28:02.937285Z","iopub.execute_input":"2023-09-11T14:28:02.937876Z","iopub.status.idle":"2023-09-11T14:28:03.616378Z","shell.execute_reply.started":"2023-09-11T14:28:02.937830Z","shell.execute_reply":"2023-09-11T14:28:03.615058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# create prompt with wikipedia information to solve problems  \ntry again! request turbo to solve problems  \nDue to the large number of characters, model with 16k(= \"gpt-3.5-turbo-16k\") are used.","metadata":{}},{"cell_type":"code","source":"answer_list=[]\nfor i in range(train.shape[0]):\n    prompt_str=train.loc[i,\"prompt\"]+\" Please choose the most accurate option from the choices A to E above and answer in the format 'The answer is A'.\"\n    prompt_str=prompt_str+'\\n----------------------------------\\n'\n    for option in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n        prompt_str=prompt_str+'Option ' + option + ' : ' + train.loc[i,option] + '\\n'\n    info_str=train.loc[i,\"wiki_info\"][:3000]\n    messages=[]\n    messages.append({\"role\": \"assistant\", \"content\": info_str})\n    messages.append({\"role\": \"user\", \"content\": prompt_str})\n\n    try:\n        response=response=response=request_gpt(\"gpt-3.5-turbo-16k\",messages)\n    except:\n        response=response=response=request_gpt(\"gpt-3.5-turbo-16k\",messages)\n    \n    if i%50==0:\n        #print sample responce\n        print(\"id\",str(i),\" responce is : \",response[\"choices\"][0][\"message\"][\"content\"])\n    \n    answer_list.append(response[\"choices\"][0][\"message\"][\"content\"][14])\n    time.sleep(1)\n    \nprint(\"done\")\ntrain[\"prediction_2\"]=answer_list\n\n#Let's take a look at the answers for the first 10 questions\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:30:10.309461Z","iopub.execute_input":"2023-09-11T14:30:10.309893Z","iopub.status.idle":"2023-09-11T14:49:58.840170Z","shell.execute_reply.started":"2023-09-11T14:30:10.309843Z","shell.execute_reply":"2023-09-11T14:49:58.838672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print sample prompt string\nprint(prompt_str)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:49:58.842798Z","iopub.execute_input":"2023-09-11T14:49:58.844013Z","iopub.status.idle":"2023-09-11T14:49:58.849949Z","shell.execute_reply.started":"2023-09-11T14:49:58.843959Z","shell.execute_reply":"2023-09-11T14:49:58.848319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check accuracy score again","metadata":{}},{"cell_type":"code","source":"print(\"accuracy score is : \",accuracy_score(train[\"answer\"],train[\"prediction_2\"]))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:49:58.851697Z","iopub.execute_input":"2023-09-11T14:49:58.852201Z","iopub.status.idle":"2023-09-11T14:49:58.872572Z","shell.execute_reply.started":"2023-09-11T14:49:58.852158Z","shell.execute_reply":"2023-09-11T14:49:58.870996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good! It is more accurate than before (score without wikipedia information = 0.72 correct response rate)！","metadata":{}},{"cell_type":"markdown","source":"# Reflection-2  \nIn a way, as expected, the accuracy was higher when the percentage of correct answers was given only for the rows to which wikipedia information could be added.  \nFrom this perhaps? What we can tell is that requesting the LLM, such as GPT3.5, with the relevant information attached to it is more accurate than requesting only the question directly to the LLM.  \nTherefore, it can be inferred that the correct response rate is greatly affected by how much correct (information necessary to derive the correct answer and related information) information is attached when making a request to an LLM model such as GPT.  \n\n**The following text has been added as an update on September 5.**  \nThe wikipedia information given in the above prompt (version with a correct response rate of 0.79) was given by extracting only the first 3,000 characters, as shown in the histogram diagram, because some of the lines have too many characters.  \nHow can we add more information than this to the \"prompt\" column?  \nAfter some searching, it seems that the langchian library and OpenAI's embedding function can be used to extract highly relevant text. I decided to do some additional work on this.","metadata":{}},{"cell_type":"markdown","source":"# install library langchain and others","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n!pip install chromadb\n!pip install tiktoken","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-11T14:49:58.875067Z","iopub.execute_input":"2023-09-11T14:49:58.875466Z","iopub.status.idle":"2023-09-11T14:51:34.584780Z","shell.execute_reply.started":"2023-09-11T14:49:58.875431Z","shell.execute_reply":"2023-09-11T14:51:34.583151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract wikipedia information related to a question using the langchain library","metadata":{"_kg_hide-output":true}},{"cell_type":"code","source":"from langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.schema.document import Document\n\ntext_splitter = CharacterTextSplitter(chunk_size=1000)\nembeddings = OpenAIEmbeddings(openai_api_key=apikey)\ndef choose_important_info(wiki_info_str,prompt_str):\n    prompt_info_str=\"\"\n    docs = [Document(page_content=x) for x in text_splitter.split_text(wiki_info_str)]\n    if len(docs)>0:\n        vectorstore = Chroma.from_documents(docs, embeddings)\n        candidate = vectorstore.similarity_search(prompt_str)\n        len_cnt=0\n        each_info=[]\n        for j in range(len(candidate)):\n            len_cnt=len_cnt+len(candidate[j].page_content)\n            each_info.append(candidate[j].page_content)\n            if(len_cnt>2000):\n                break\n        prompt_info_str = '\\n'.join(list(set(each_info)))\n    else:\n        prompt_info_str=\"\"\n    return prompt_info_str","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:51:34.587440Z","iopub.execute_input":"2023-09-11T14:51:34.588034Z","iopub.status.idle":"2023-09-11T14:51:37.892108Z","shell.execute_reply.started":"2023-09-11T14:51:34.587978Z","shell.execute_reply":"2023-09-11T14:51:37.890651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# create prompt with Wikipedia information highly relevant to the question","metadata":{}},{"cell_type":"code","source":"answer_list=[]\nfor i in range(train.shape[0]):\n    prompt_str=train.loc[i,\"prompt\"]+\" Please choose the most accurate option from the choices A to E above and answer in the format 'The answer is A'.\"\n    prompt_str=prompt_str+'\\n----------------------------------\\n'\n    for option in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n        prompt_str=prompt_str+'Option ' + option + ' : ' + train.loc[i,option] + '\\n'\n    info_str=choose_important_info(train.loc[i,\"wiki_info\"],train.loc[i,\"prompt\"])\n    messages=[]\n    messages.append({\"role\": \"assistant\", \"content\": info_str})\n    messages.append({\"role\": \"user\", \"content\": prompt_str[:2000]})\n\n    try:\n        response=response=response=request_gpt(\"gpt-3.5-turbo-16k\",messages)\n    except:\n        response=response=response=request_gpt(\"gpt-3.5-turbo-16k\",messages)\n    \n    if i%50==0:\n        #print sample prompt and responce\n        print(\"prompt message is :\",messages)\n        print(\"id\",str(i),\" responce is : \",response[\"choices\"][0][\"message\"][\"content\"])\n    \n    answer_list.append(response[\"choices\"][0][\"message\"][\"content\"][14])\n    time.sleep(1)\n    \nprint(\"done\")\ntrain[\"prediction_3\"]=answer_list\n\n#Let's take a look at the answers for the first 10 questions\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T14:51:37.893429Z","iopub.execute_input":"2023-09-11T14:51:37.893767Z","iopub.status.idle":"2023-09-11T15:09:47.743198Z","shell.execute_reply.started":"2023-09-11T14:51:37.893737Z","shell.execute_reply":"2023-09-11T15:09:47.741837Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check accuracy score again","metadata":{}},{"cell_type":"code","source":"print(\"accuracy score is : \",accuracy_score(train[\"answer\"],train[\"prediction_3\"]))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:47.744728Z","iopub.execute_input":"2023-09-11T15:09:47.745093Z","iopub.status.idle":"2023-09-11T15:09:47.753251Z","shell.execute_reply.started":"2023-09-11T15:09:47.745059Z","shell.execute_reply":"2023-09-11T15:09:47.751880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's right on target! I was able to further increase the percentage of correct answers!","metadata":{}},{"cell_type":"markdown","source":"# Reflection-3  \nAt first, the GPT generates answers very simply, using only the questions, choices, and a few additional request sentences prepared in the training data set.  \nNext, additional information is added and answers are generated by GPT.  \nNext, additional information is added to the training data set and the GPT is used to generate answers.  \nI divided the process into three phases and checked how the percentage of correct answers changed.  \nAs I had hoped, I was able to increase the correct answer rate from 0.72 to 0.785 to 0.84.  \nIn order to increase the percentage of correct answers, perhaps? Although I was able to examine the necessary information carefully, I have not yet been able to edit it for submission, so I will consider this issue from now on.  \nAlso, in order to execute this notebook, it is necessary to obtain the API-KEY of OpenAI, but I would like to consider whether the same process can be done with another LLM (preferably one that does not require API-KEY, etc.).","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}