{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install packages","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/nh-llama-2-7b/accelerate-0.21.0-py3-none-any.whl\n!pip install /kaggle/input/nh-llama-2-7b/bitsandbytes-0.41.1-py3-none-any.whl\n!pip install /kaggle/input/nh-llama-2-7b/peft-0.4.0-py3-none-any.whl\n!pip install /kaggle/input/nh-llama-2-7b/trl-0.5.0-py3-none-any.whl\n!pip install /kaggle/input/nh-llama-2-7b/openapi_schema_pydantic-1.2.4-py3-none-any.whl\n!pip install /kaggle/input/nh-llama-2-7b/langsmith-0.0.22-py3-none-any.whl\n!pip install /kaggle/input/nh-llama-2-7b/langchain-0.0.264-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.955997Z","iopub.status.idle":"2023-08-14T12:15:48.956836Z","shell.execute_reply.started":"2023-08-14T12:15:48.956555Z","shell.execute_reply":"2023-08-14T12:15:48.956579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport pandas as pd\nfrom string import Template\nfrom pathlib import Path\n\n\nimport os\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nfrom tqdm.notebook import tqdm\n\n# for training\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n# for traing set\nfrom datasets import load_dataset\nfrom langchain.prompts import PromptTemplate\nimport matplotlib.pyplot as plt\nimport bitsandbytes as bnb\nimport numpy as np\n\nfrom IPython.display import Markdown, display","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.958386Z","iopub.status.idle":"2023-08-14T12:15:48.959301Z","shell.execute_reply.started":"2023-08-14T12:15:48.959034Z","shell.execute_reply":"2023-08-14T12:15:48.95906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load model and tokenizer","metadata":{}},{"cell_type":"code","source":"# change model_name to the model of your choice.\n# This can be either name of the model on huggingface (requires internet) or path to the model\nmodel_name = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtyp=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True\n)\n# this should be set as False for finetuning\nmodel.config.use_cache = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.960944Z","iopub.status.idle":"2023-08-14T12:15:48.961913Z","shell.execute_reply.started":"2023-08-14T12:15:48.9616Z","shell.execute_reply":"2023-08-14T12:15:48.961627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prepare training data","metadata":{}},{"cell_type":"code","source":"# load training data\ntrain_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/kaggle-llm-science-exam/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.973161Z","iopub.status.idle":"2023-08-14T12:15:48.974067Z","shell.execute_reply.started":"2023-08-14T12:15:48.973772Z","shell.execute_reply":"2023-08-14T12:15:48.973817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare template \ntemplate = \"\"\"Answer the following multiple choice question by giving the most appropriate response. Answer should be one among [A, B, C, D, E]\n\nQuestion: {prompt}\\n\nA) {a}\\n\nB) {b}\\n\nC) {c}\\n\nD) {d}\\n\nE) {e}\\n\n\n### Answer: {answer}\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=['prompt', 'a', 'b', 'c', 'd', 'e', 'answer'])","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.975665Z","iopub.status.idle":"2023-08-14T12:15:48.97657Z","shell.execute_reply.started":"2023-08-14T12:15:48.976291Z","shell.execute_reply":"2023-08-14T12:15:48.976317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display sample to see template\nsample = train_dataset['train'][0]\ndisplay(Markdown(prompt.format(prompt=sample['prompt'], \n                               a=sample['A'], \n                               b=sample['B'], \n                               c=sample['C'], \n                               d=sample['D'], \n                               e=sample['E'], \n                               answer=sample['answer'])))","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.978154Z","iopub.status.idle":"2023-08-14T12:15:48.97904Z","shell.execute_reply.started":"2023-08-14T12:15:48.978723Z","shell.execute_reply":"2023-08-14T12:15:48.97879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_text(example):\n    \"\"\" fill inputs in promt for a sample  \"\"\"\n    text = prompt.format(prompt=example['prompt'], \n                         a=example['A'], \n                         b=example['B'], \n                         c=example['C'], \n                         d=example['D'], \n                         e=example['E'], \n                         answer=example['answer'])\n    return {\"text\": text}","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.980458Z","iopub.status.idle":"2023-08-14T12:15:48.981262Z","shell.execute_reply.started":"2023-08-14T12:15:48.981014Z","shell.execute_reply":"2023-08-14T12:15:48.981038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(format_text)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.982665Z","iopub.status.idle":"2023-08-14T12:15:48.983499Z","shell.execute_reply.started":"2023-08-14T12:15:48.983239Z","shell.execute_reply":"2023-08-14T12:15:48.983264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set up training arguments","metadata":{}},{"cell_type":"code","source":"# check model structure\nmodel","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_linear_layers(model):\n    \"\"\" find linear layers in given transformer model \"\"\"\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        # 4 bits for qlora\n        if isinstance(module, bnb.nn.Linear4bit): \n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    print(f\"LoRA module names: {list(lora_module_names)}\")\n    return list(lora_module_names)\n\n\ntarget_modules = find_linear_layers(model)\n#for llama 2 (they need different target module)\nqlora_config = LoraConfig(\n    r=16,  # dimension of the updated matrices\n    lora_alpha=64,  # parameter for scaling\n    target_modules=target_modules, # this chooses on which layers QLoRA is applied\n    lora_dropout=0.1,  # dropout probability for layers\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.985081Z","iopub.status.idle":"2023-08-14T12:15:48.985936Z","shell.execute_reply.started":"2023-08-14T12:15:48.985655Z","shell.execute_reply":"2023-08-14T12:15:48.985679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"max_steps=1\" is just for testing execution\ntraining_args = TrainingArguments(\n    output_dir=\"./SFT-llama2-7b\", \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    logging_steps=20,\n    logging_strategy=\"steps\",\n    warmup_steps=2,\n#     num_train_epochs=1,\n    max_steps=1,\n    optim=\"paged_adamw_8bit\",\n    fp16=True,\n    run_name=\"baseline-llama2-sft\",\n    save_total_limit=1,  # can be increased, but but beware of kaggle notebook output size limit\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.987413Z","iopub.status.idle":"2023-08-14T12:15:48.988246Z","shell.execute_reply.started":"2023-08-14T12:15:48.988001Z","shell.execute_reply":"2023-08-14T12:15:48.988024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"supervised_finetuning_trainer = SFTTrainer(\n    model,\n    train_dataset=train_dataset['train'],\n    args=training_args,\n    tokenizer=tokenizer,\n    peft_config=qlora_config,\n    dataset_text_field=\"text\",\n    max_seq_length=3000,\n    data_collator=DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, \n                                                  response_template=\"Answer:\")\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.989648Z","iopub.status.idle":"2023-08-14T12:15:48.990491Z","shell.execute_reply.started":"2023-08-14T12:15:48.990228Z","shell.execute_reply":"2023-08-14T12:15:48.990253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"supervised_finetuning_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.992076Z","iopub.status.idle":"2023-08-14T12:15:48.99294Z","shell.execute_reply.started":"2023-08-14T12:15:48.992665Z","shell.execute_reply":"2023-08-14T12:15:48.992691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"model_to_save = supervised_finetuning_trainer.model.module if hasattr(supervised_finetuning_trainer.model, 'module') else supervised_finetuning_trainer.model\nmodel_to_save.save_pretrained(\"outputs\")","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.994388Z","iopub.status.idle":"2023-08-14T12:15:48.995253Z","shell.execute_reply.started":"2023-08-14T12:15:48.994988Z","shell.execute_reply":"2023-08-14T12:15:48.995012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying lora","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig.from_pretrained('outputs')\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:15:48.996724Z","iopub.status.idle":"2023-08-14T12:15:48.997519Z","shell.execute_reply.started":"2023-08-14T12:15:48.997272Z","shell.execute_reply":"2023-08-14T12:15:48.997296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission","metadata":{}},{"cell_type":"markdown","source":"### Prepare test set","metadata":{}},{"cell_type":"code","source":"# same prompt as before\ntemplate = \"\"\"Answer the following multiple choice question by giving the most appropriate response. Answer should be one among [A, B, C, D, E]\n\nQuestion: {prompt}\\n\nA) {a}\\n\nB) {b}\\n\nC) {c}\\n\nD) {d}\\n\nE) {e}\\n\n\n### Answer: {answer}\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=['prompt', 'a', 'b', 'c', 'd', 'e', 'answer'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We don't have answers for test\ndef format_text_test(example):\n    text = prompt.format(prompt=example['prompt'], \n                         a=example['A'], \n                         b=example['B'], \n                         c=example['C'], \n                         d=example['D'], \n                         e=example['E'], \n                         answer='')\n    return {\"text\": text}\n\n\ntest_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ntest_dataset = test_dataset.map(format_text_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict with fine-tuned model","metadata":{}},{"cell_type":"code","source":"from torch import nn\nclass Perplexity(nn.Module):\n    def __init__(self, reduce: bool = True):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.reduce = reduce\n\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n\n        perplexity = []\n        for i in range(labels.shape[0]):\n            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n        perplexity = torch.stack(perplexity, dim=0)\n        if self.reduce:\n            perplexity = torch.mean(perplexity)\n        return perplexity \n    \nperp = Perplexity()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor idx in tqdm(range(len(test_dataset[\"train\"])), total=len(test_dataset[\"train\"])):\n    \n    with torch.no_grad():\n        cols = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n        perps = []\n        samples = []\n        for col in cols:\n            prompt = test_dataset['train'][idx]['text']\n            samples.append(prompt + col)\n        inputs = tokenizer(samples, return_tensors=\"pt\", add_special_tokens=False, padding=True, truncation=True).to(\"cuda\")\n\n        output = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n        output = output.logits\n        labels = inputs[\"input_ids\"]\n        labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n        for j in range(len(cols)):\n            p = perp(output[j].unsqueeze(0), labels[j].unsqueeze(0))\n            perps.append(p.detach().cpu())\n            \n        del inputs\n        del labels\n        del output\n        del p\n\n    perps = np.array(perps)\n    predictions = [np.array(cols)[np.argsort(perps)]]\n    preds.append(predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### format predictions to sumbission format and save","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:39:01.937215Z","iopub.execute_input":"2023-08-19T13:39:01.937617Z","iopub.status.idle":"2023-08-19T13:39:01.944443Z","shell.execute_reply.started":"2023-08-19T13:39:01.937586Z","shell.execute_reply":"2023-08-19T13:39:01.942939Z"}}},{"cell_type":"code","source":"def format_prediction(row, k=3):\n    best_k_preds = row[0][:k]\n    return ' '.join(best_k_preds)\n\ntest_df = pd.DataFrame(preds)\nformat_prediction(test_df.iloc[0, :])\ntest_df['prediction'] = test_df.apply(lambda x: format_prediction(x), axis=1)\ntest_df['id'] = test_df.index\n\nsubmission = test_df[['id', 'prediction']]\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}